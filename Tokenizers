'''Opening sample.txt - Take the sample.txt file as your own example to try by yourself.'''
f=open("sample.txt")
raw=f.read()
print(raw)
--------------------------------------------------------------------------------------------------------------------------------------------------------
Food is any substance consumed to provide nutritional support for an organism. Food is usually of plant, animal or fungal origin, and contains essential nutrients, such as carbohydrates, fats, proteins, vitamins, or minerals. The substance is ingested by an organism and assimilated by the organism's cells to provide energy, maintain life, or stimulate growth. Different species of animals have different feeding behaviours that satisfy the needs of their unique metabolisms, often evolved to fill a specific ecological niche within specific geographical contexts.
--------------------------------------------------------------------------------------------------------------------------------------------------------

'''Importing Packages'''
import nltk
from nltk.tokenize import sent_tokenize,word_tokenize

'''For sentence and word tokenizer'''
nltk.download('punkt')
print(sent_tokenize(raw))
--------------------------------------------------------------------------------------------------------------------------------------------------------
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
['Food is any substance consumed to provide nutritional support for an organism.', 'Food is usually of plant, animal or fungal origin, and contains essential nutrients, such as carbohydrates, fats, proteins, vitamins, or minerals.', "The substance is ingested by an organism and assimilated by the organism's cells to provide energy, maintain life, or stimulate growth.", 'Different species of animals have different feeding behaviours that satisfy the needs of their unique metabolisms, often evolved to fill a specific ecological niche within specific geographical contexts.']
--------------------------------------------------------------------------------------------------------------------------------------------------------

words=word_tokenize(raw)
print(words)
--------------------------------------------------------------------------------------------------------------------------------------------------------
['Food', 'is', 'any', 'substance', 'consumed', 'to', 'provide', 'nutritional', 'support', 'for', 'an', 'organism', '.', 'Food', 'is', 'usually', 'of', 'plant', ',', 'animal', 'or', 'fungal', 'origin', ',', 'and', 'contains', 'essential', 'nutrients', ',', 'such', 'as', 'carbohydrates', ',', 'fats', ',', 'proteins', ',', 'vitamins', ',', 'or', 'minerals', '.', 'The', 'substance', 'is', 'ingested', 'by', 'an', 'organism', 'and', 'assimilated', 'by', 'the', 'organism', "'s", 'cells', 'to', 'provide', 'energy', ',', 'maintain', 'life', ',', 'or', 'stimulate', 'growth', '.', 'Different', 'species', 'of', 'animals', 'have', 'different', 'feeding', 'behaviours', 'that', 'satisfy', 'the', 'needs', 'of', 'their', 'unique', 'metabolisms', ',', 'often', 'evolved', 'to', 'fill', 'a', 'specific', 'ecological', 'niche', 'within', 'specific', 'geographical', 'contexts', '.']
--------------------------------------------------------------------------------------------------------------------------------------------------------

from nltk.tokenize import TreebankWordTokenizer
tokenizer=TreebankWordTokenizer()
tokenizer.tokenize(raw)
--------------------------------------------------------------------------------------------------------------------------------------------------------
['Food',
 'is',
 'any',
 'substance',
 'consumed',
 'to',
 'provide',
 'nutritional',
 'support',
 'for',
 'an',
 'organism.',
 'Food',
 'is',
 'usually',
 'of',
 'plant',
 ',',
 'animal',
 'or',
 'fungal',
 'origin',
 ',',
 'and',
 'contains',
 'essential',
 'nutrients',
 ',',
 'such',
 'as',
 'carbohydrates',
 ',',
 'fats',
 ',',
 'proteins',
 ',',
 'vitamins',
 ',',
 'or',
 'minerals.',
 'The',
 'substance',
 'is',
 'ingested',
 'by',
 'an',
 'organism',
 'and',
 'assimilated',
 'by',
 'the',
 'organism',
 "'s",
 'cells',
 'to',
 'provide',
 'energy',
 ',',
 'maintain',
 'life',
 ',',
 'or',
 'stimulate',
 'growth.',
 'Different',
 'species',
 'of',
 'animals',
 'have',
 'different',
 'feeding',
 'behaviours',
 'that',
 'satisfy',
 'the',
 'needs',
 'of',
 'their',
 'unique',
 'metabolisms',
 ',',
 'often',
 'evolved',
 'to',
 'fill',
 'a',
 'specific',
 'ecological',
 'niche',
 'within',
 'specific',
 'geographical',
 'contexts',
 '.']
--------------------------------------------------------------------------------------------------------------------------------------------------------

from nltk.tokenize import WordPunctTokenizer
tokenizer=WordPunctTokenizer()
tokenizer.tokenize(raw)
--------------------------------------------------------------------------------------------------------------------------------------------------------
['Food',
 'is',
 'any',
 'substance',
 'consumed',
 'to',
 'provide',
 'nutritional',
 'support',
 'for',
 'an',
 'organism',
 '.',
 'Food',
 'is',
 'usually',
 'of',
 'plant',
 ',',
 'animal',
 'or',
 'fungal',
 'origin',
 ',',
 'and',
 'contains',
 'essential',
 'nutrients',
 ',',
 'such',
 'as',
 'carbohydrates',
 ',',
 'fats',
 ',',
 'proteins',
 ',',
 'vitamins',
 ',',
 'or',
 'minerals',
 '.',
 'The',
 'substance',
 'is',
 'ingested',
 'by',
 'an',
 'organism',
 'and',
 'assimilated',
 'by',
 'the',
 'organism',
 "'",
 's',
 'cells',
 'to',
 'provide',
 'energy',
 ',',
 'maintain',
 'life',
 ',',
 'or',
 'stimulate',
 'growth',
 '.',
 'Different',
 'species',
 'of',
 'animals',
 'have',
 'different',
 'feeding',
 'behaviours',
 'that',
 'satisfy',
 'the',
 'needs',
 'of',
 'their',
 'unique',
 'metabolisms',
 ',',
 'often',
 'evolved',
 'to',
 'fill',
 'a',
 'specific',
 'ecological',
 'niche',
 'within',
 'specific',
 'geographical',
 'contexts',
 '.']
--------------------------------------------------------------------------------------------------------------------------------------------------------

from nltk.tokenize import PunktSentenceTokenizer
tokenizer=PunktSentenceTokenizer()
tokenizer.tokenize(raw)
--------------------------------------------------------------------------------------------------------------------------------------------------------
['Food is any substance consumed to provide nutritional support for an organism.',
 'Food is usually of plant, animal or fungal origin, and contains essential nutrients, such as carbohydrates, fats, proteins, vitamins, or minerals.',
 "The substance is ingested by an organism and assimilated by the organism's cells to provide energy, maintain life, or stimulate growth.",
 'Different species of animals have different feeding behaviours that satisfy the needs of their unique metabolisms, often evolved to fill a specific ecological niche within specific geographical contexts.']
--------------------------------------------------------------------------------------------------------------------------------------------------------

#Separating words using regular expression ,\w -words \w'- words with apostrophe letters also ,\s-strings ,+ represents one or more words
from nltk.tokenize import RegexpTokenizer
tokenizer=RegexpTokenizer("[\w']+")
tokenizer.tokenize(raw)
--------------------------------------------------------------------------------------------------------------------------------------------------------
['Food',
 'is',
 'any',
 'substance',
 'consumed',
 'to',
 'provide',
 'nutritional',
 'support',
 'for',
 'an',
 'organism',
 'Food',
 'is',
 'usually',
 'of',
 'plant',
 'animal',
 'or',
 'fungal',
 'origin',
 'and',
 'contains',
 'essential',
 'nutrients',
 'such',
 'as',
 'carbohydrates',
 'fats',
 'proteins',
 'vitamins',
 'or',
 'minerals',
 'The',
 'substance',
 'is',
 'ingested',
 'by',
 'an',
 'organism',
 'and',
 'assimilated',
 'by',
 'the',
 "organism's",
 'cells',
 'to',
 'provide',
 'energy',
 'maintain',
 'life',
 'or',
 'stimulate',
 'growth',
 'Different',
 'species',
 'of',
 'animals',
 'have',
 'different',
 'feeding',
 'behaviours',
 'that',
 'satisfy',
 'the',
 'needs',
 'of',
 'their',
 'unique',
 'metabolisms',
 'often',
 'evolved',
 'to',
 'fill',
 'a',
 'specific',
 'ecological',
 'niche',
 'within',
 'specific',
 'geographical',
 'contexts']
--------------------------------------------------------------------------------------------------------------------------------------------------------

from nltk.tokenize import RegexpTokenizer
tokenizer=RegexpTokenizer("\s+",gaps=True)
tokenizer.tokenize(raw)
--------------------------------------------------------------------------------------------------------------------------------------------------------
['Food',
 'is',
 'any',
 'substance',
 'consumed',
 'to',
 'provide',
 'nutritional',
 'support',
 'for',
 'an',
 'organism.',
 'Food',
 'is',
 'usually',
 'of',
 'plant,',
 'animal',
 'or',
 'fungal',
 'origin,',
 'and',
 'contains',
 'essential',
 'nutrients,',
 'such',
 'as',
 'carbohydrates,',
 'fats,',
 'proteins,',
 'vitamins,',
 'or',
 'minerals.',
 'The',
 'substance',
 'is',
 'ingested',
 'by',
 'an',
 'organism',
 'and',
 'assimilated',
 'by',
 'the',
 "organism's",
 'cells',
 'to',
 'provide',
 'energy,',
 'maintain',
 'life,',
 'or',
 'stimulate',
 'growth.',
 'Different',
 'species',
 'of',
 'animals',
 'have',
 'different',
 'feeding',
 'behaviours',
 'that',
 'satisfy',
 'the',
 'needs',
 'of',
 'their',
 'unique',
 'metabolisms,',
 'often',
 'evolved',
 'to',
 'fill',
 'a',
 'specific',
 'ecological',
 'niche',
 'within',
 'specific',
 'geographical',
 'contexts.']
--------------------------------------------------------------------------------------------------------------------------------------------------------

#Processing tweets directly using TweetTokenizer
sentence="I had a GREAT week,thanks to YOU!If you need anything,please reach out.  ❤️  ❤️  ❤️#WorldSmileDay"
from nltk.tokenize import TweetTokenizer
tokenizer=TweetTokenizer()
tokenizer.tokenize(sentence)
--------------------------------------------------------------------------------------------------------------------------------------------------------
['I',
 'had',
 'a',
 'GREAT',
 'week',
 ',',
 'thanks',
 'to',
 'YOU',
 '!',
 'If',
 'you',
 'need',
 'anything',
 ',',
 'please',
 'reach',
 'out',
 '.',
 '❤',
 '️',
 '❤',
 '️',
 '❤',
 '️',
 '#WorldSmileDay']
--------------------------------------------------------------------------------------------------------------------------------------------------------

sentence="Good performance by Indian Army on Republic Day"
from nltk.tokenize import MWETokenizer
mwe_tokenizer=MWETokenizer([('Republic','Day'),('Indian','Army')])
mwe_tokenizer.add_mwe(['Indian','Army'])
mwe_tokenizer.tokenize(sentence.split())
--------------------------------------------------------------------------------------------------------------------------------------------------------
['Good', 'performance', 'by', 'Indian_Army', 'on', 'Republic_Day']
--------------------------------------------------------------------------------------------------------------------------------------------------------
